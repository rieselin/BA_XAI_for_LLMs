
@misc{noauthor_github_2025,
	title = {{GitHub} {Copilot} · {Your} {AI} pair programmer},
	url = {https://github.com/features/copilot},
	abstract = {GitHub Copilot works alongside you directly in your editor, suggesting whole lines or entire functions for you.},
	language = {en},
	urldate = {2025-12-07},
	journal = {GitHub},
	year = {2025},
}

@misc{noauthor_claude_nodate,
	title = {Claude},
	url = {https://claude.ai/new},
	abstract = {Talk with Claude, an AI assistant from Anthropic},
	language = {en-US},
	urldate = {2025-12-07},
}

@misc{noauthor_chatgpt_nodate,
	title = {{ChatGPT}},
	url = {https://chatgpt.com/},
	abstract = {ChatGPT helps you get answers, find inspiration, and be more productive.},
	language = {en-US},
	urldate = {2025-12-07},
	journal = {ChatGPT},
}

@misc{arnold_explainable_2024,
	title = {Explainable {Search} and {Discovery} of {Visual} {Cultural} {Heritage} {Collections} with {Multimodal} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2411.04663},
	doi = {10.48550/arXiv.2411.04663},
	abstract = {Many cultural institutions have made large digitized visual collections available online, often under permissible re-use licences. Creating interfaces for exploring and searching these collections is difficult, particularly in the absence of granular metadata. In this paper, we introduce a method for using state-of-the-art multimodal large language models (LLMs) to enable an open-ended, explainable search and discovery interface for visual collections. We show how our approach can create novel clustering and recommendation systems that avoid common pitfalls of methods based directly on visual embeddings. Of particular interest is the ability to offer concrete textual explanations of each recommendation without the need to preselect the features of interest. Together, these features can create a digital interface that is more open-ended and flexible while also being better suited to addressing privacy and ethical concerns. Through a case study using a collection of documentary photographs, we provide several metrics showing the efficacy and possibilities of our approach.},
	urldate = {2025-12-02},
	publisher = {arXiv},
	author = {Arnold, Taylor and Tilton, Lauren},
	month = nov,
	year = {2024},
	note = {arXiv:2411.04663 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{cho_language-conditioned_2024,
	address = {Seattle, WA, USA},
	title = {Language-{Conditioned} {Detection} {Transformer}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-5300-6},
	url = {https://ieeexplore.ieee.org/document/10655673/},
	doi = {10.1109/CVPR52733.2024.01570},
	abstract = {We present a new open-vocabulary detection framework. Our framework uses both image-level labels and detailed detection annotations when available. Our framework proceeds in three steps. We first train a language-conditioned object detector on fully-supervised detection data. This detector gets to see the presence or absence of ground truth classes during training, and conditions prediction on the set of present classes. We use this detector to pseudolabel images with image-level labels. Our detector provides much more accurate pseudo-labels than prior approaches with its conditioning mechanism. Finally, we train an unconditioned open-vocabulary detector on the pseudo-annotated images. The resulting detector, named DECOLA, shows strong zero-shot performance in openvocabulary LVIS benchmark as well as direct zero-shot transfer benchmarks on LVIS, COCO, Object365, and OpenImages. DECOLA outperforms the prior arts by 17.1 APrare and 9.4 mAP on zero-shot LVIS benchmark. DECOLA achieves state-of-the-art results in various model sizes, architectures, and datasets by only training on open-sourced data and academic-scale computing. Code is available at https://github.com/janghyuncho/DECOLA.},
	language = {en},
	urldate = {2025-12-02},
	booktitle = {2024 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Cho, Jang Hyun and Krähenbühl, Philipp},
	month = jun,
	year = {2024},
	pages = {16593--16603},
}

@misc{zhao_odam_2023,
	title = {{ODAM}: {Gradient}-based instance-specific visual explanations for object detection},
	shorttitle = {{ODAM}},
	url = {http://arxiv.org/abs/2304.06354},
	doi = {10.48550/arXiv.2304.06354},
	abstract = {We propose the gradient-weighted Object Detector Activation Maps (ODAM), a visualized explanation technique for interpreting the predictions of object detectors. Utilizing the gradients of detector targets flowing into the intermediate feature maps, ODAM produces heat maps that show the influence of regions on the detector's decision for each predicted attribute. Compared to previous works classification activation maps (CAM), ODAM generates instance-specific explanations rather than class-specific ones. We show that ODAM is applicable to both one-stage detectors and two-stage detectors with different types of detector backbones and heads, and produces higher-quality visual explanations than the state-of-the-art both effectively and efficiently. We next propose a training scheme, Odam-Train, to improve the explanation ability on object discrimination of the detector through encouraging consistency between explanations for detections on the same object, and distinct explanations for detections on different objects. Based on the heat maps produced by ODAM with Odam-Train, we propose Odam-NMS, which considers the information of the model's explanation for each prediction to distinguish the duplicate detected objects. We present a detailed analysis of the visualized explanations of detectors and carry out extensive experiments to validate the effectiveness of the proposed ODAM.},
	urldate = {2025-12-02},
	publisher = {arXiv},
	author = {Zhao, Chenyang and Chan, Antoni B.},
	month = apr,
	year = {2023},
	note = {arXiv:2304.06354 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{chen_introduction_2023,
	title = {Introduction of an intriguing approach for eletric current transformer on-site examining repairing},
	url = {http://arxiv.org/abs/2304.08325},
	doi = {10.48550/arXiv.2304.08325},
	abstract = {The working principle of a electric current transformer is based on electromagnetic induction, mainly composed of a closed iron core and windings. Its primary winding has relatively few turns and is connected in series with the current circuit to be measured. However, due to the frequent occurrence of full current passing through the current transformer during use, its secondary winding has relatively more turns. During use, errors may occur in the current transformer. Therefore, technical personnel can adopt the heterodyne measurement method to shield interference signals and ensure the accuracy of measurements during on-site examining and repairing of current transformers. This article mainly introduces the characteristics of transformers and their on-site examining and repairing process and errors, and describes the specific application of heterodyne measurement in on-site examining and repairing of current transformers.},
	urldate = {2025-11-28},
	publisher = {arXiv},
	author = {Chen, Yuxuan and Sun, Jing and Meng, Boqi},
	month = apr,
	year = {2023},
	note = {arXiv:2304.08325 [eess]},
	keywords = {Electrical Engineering and Systems Science - Systems and Control},
}

@inproceedings{colnet_complexity_2022,
	title = {On the {Complexity} of {Enumerating} {Prime} {Implicants} from {Decision}-{DNNF} {Circuits}},
	url = {http://arxiv.org/abs/2301.13328},
	doi = {10.24963/ijcai.2022/358},
	abstract = {We consider the problem EnumIP of enumerating prime implicants of Boolean functions represented by decision decomposable negation normal form (dec-DNNF) circuits. We study EnumIP from dec-DNNF within the framework of enumeration complexity and prove that it is in OutputP, the class of output polynomial enumeration problems, and more precisely in IncP, the class of polynomial incremental time enumeration problems. We then focus on two closely related, but seemingly harder, enumeration problems where further restrictions are put on the prime implicants to be generated. In the first problem, one is only interested in prime implicants representing subset-minimal abductive explanations, a notion much investigated in AI for more than three decades. In the second problem, the target is prime implicants representing sufficient reasons, a recent yet important notion in the emerging field of eXplainable AI, since they aim to explain predictions achieved by machine learning classifiers. We provide evidence showing that enumerating specific prime implicants corresponding to subset-minimal abductive explanations or to sufficient reasons is not in OutputP.},
	urldate = {2025-11-28},
	booktitle = {Proceedings of the {Thirty}-{First} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Colnet, Alexis de and Marquis, Pierre},
	month = jul,
	year = {2022},
	note = {arXiv:2301.13328 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	pages = {2583--2590},
}

@misc{borgeaud_improving_2022,
	title = {Improving language models by retrieving from trillions of tokens},
	url = {http://arxiv.org/abs/2112.04426},
	doi = {10.48550/arXiv.2112.04426},
	abstract = {We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a \$2\$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25\${\textbackslash}times\$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.},
	urldate = {2025-11-28},
	publisher = {arXiv},
	author = {Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Driessche, George van den and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and Casas, Diego de Las and Guy, Aurelia and Menick, Jacob and Ring, Roman and Hennigan, Tom and Huang, Saffron and Maggiore, Loren and Jones, Chris and Cassirer, Albin and Brock, Andy and Paganini, Michela and Irving, Geoffrey and Vinyals, Oriol and Osindero, Simon and Simonyan, Karen and Rae, Jack W. and Elsen, Erich and Sifre, Laurent},
	month = feb,
	year = {2022},
	note = {arXiv:2112.04426 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{li_blip_2022,
	title = {{BLIP}: {Bootstrapping} {Language}-{Image} {Pre}-training for {Unified} {Vision}-{Language} {Understanding} and {Generation}},
	shorttitle = {{BLIP}},
	url = {http://arxiv.org/abs/2201.12086},
	doi = {10.48550/arXiv.2201.12086},
	abstract = {Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7\% in average recall@1), image captioning (+2.8\% in CIDEr), and VQA (+1.6\% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to video-language tasks in a zero-shot manner. Code, models, and datasets are released at https://github.com/salesforce/BLIP.},
	urldate = {2025-11-28},
	publisher = {arXiv},
	author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
	month = feb,
	year = {2022},
	note = {arXiv:2201.12086 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{radford_learning_2021,
	title = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
	url = {http://arxiv.org/abs/2103.00020},
	doi = {10.48550/arXiv.2103.00020},
	abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
	urldate = {2025-11-28},
	publisher = {arXiv},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	month = feb,
	year = {2021},
	note = {arXiv:2103.00020 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{girshick_rich_2014,
	title = {Rich feature hierarchies for accurate object detection and semantic segmentation},
	url = {http://arxiv.org/abs/1311.2524},
	doi = {10.48550/arXiv.1311.2524},
	abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012---achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/{\textasciitilde}rbg/rcnn.},
	urldate = {2025-11-28},
	publisher = {arXiv},
	author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
	month = oct,
	year = {2014},
	note = {arXiv:1311.2524 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{he_mask_2018,
	title = {Mask {R}-{CNN}},
	url = {http://arxiv.org/abs/1703.06870},
	doi = {10.48550/arXiv.1703.06870},
	abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
	urldate = {2025-11-28},
	publisher = {arXiv},
	author = {He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross},
	month = jan,
	year = {2018},
	note = {arXiv:1703.06870 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{lin_focal_2018,
	title = {Focal {Loss} for {Dense} {Object} {Detection}},
	url = {http://arxiv.org/abs/1708.02002},
	doi = {10.48550/arXiv.1708.02002},
	abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
	urldate = {2025-11-28},
	publisher = {arXiv},
	author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollár, Piotr},
	month = feb,
	year = {2018},
	note = {arXiv:1708.02002 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@incollection{liu_ssd_2016,
	title = {{SSD}: {Single} {Shot} {MultiBox} {Detector}},
	volume = {9905},
	shorttitle = {{SSD}},
	url = {http://arxiv.org/abs/1512.02325},
	abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For \$300{\textbackslash}times 300\$ input, SSD achieves 72.1\% mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for \$500{\textbackslash}times 500\$ input, SSD achieves 75.1\% mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at https://github.com/weiliu89/caffe/tree/ssd .},
	urldate = {2025-11-28},
	author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
	year = {2016},
	doi = {10.1007/978-3-319-46448-0_2},
	note = {arXiv:1512.02325 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {21--37},
}

@misc{ultralytics_entdecken_nodate,
	title = {Entdecken {Sie} {Ultralytics} {YOLOv8}},
	url = {https://docs.ultralytics.com/de/models/yolov8/},
	abstract = {Entdecken Sie Ultralytics YOLOv8, eine Weiterentwicklung der Echtzeit-Objekterkennung, die die Leistung mit einer Reihe vortrainierter Modelle für verschiedene Aufgaben optimiert.},
	language = {de},
	urldate = {2025-11-28},
	author = {Ultralytics},
}

@misc{carion_end--end_2020,
	title = {End-to-{End} {Object} {Detection} with {Transformers}},
	url = {http://arxiv.org/abs/2005.12872},
	doi = {10.48550/arXiv.2005.12872},
	abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
	urldate = {2025-11-24},
	publisher = {arXiv},
	author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
	month = may,
	year = {2020},
	note = {arXiv:2005.12872 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{petsiuk_black-box_2021,
	title = {Black-box {Explanation} of {Object} {Detectors} via {Saliency} {Maps}},
	url = {http://arxiv.org/abs/2006.03204},
	doi = {10.48550/arXiv.2006.03204},
	abstract = {We propose D-RISE, a method for generating visual explanations for the predictions of object detectors. Utilizing the proposed similarity metric that accounts for both localization and categorization aspects of object detection allows our method to produce saliency maps that show image areas that most affect the prediction. D-RISE can be considered "black-box" in the software testing sense, as it only needs access to the inputs and outputs of an object detector. Compared to gradient-based methods, D-RISE is more general and agnostic to the particular type of object detector being tested, and does not need knowledge of the inner workings of the model. We show that D-RISE can be easily applied to different object detectors including one-stage detectors such as YOLOv3 and two-stage detectors such as Faster-RCNN. We present a detailed analysis of the generated visual explanations to highlight the utilization of context and possible biases learned by object detectors.},
	urldate = {2025-11-24},
	publisher = {arXiv},
	author = {Petsiuk, Vitali and Jain, Rajiv and Manjunatha, Varun and Morariu, Vlad I. and Mehra, Ashutosh and Ordonez, Vicente and Saenko, Kate},
	month = jun,
	year = {2021},
	note = {arXiv:2006.03204 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{andres_black-box_2024,
	title = {On the black-box explainability of object detection models for safe and trustworthy industrial applications},
	volume = {24},
	issn = {2590-1230},
	url = {https://www.sciencedirect.com/science/article/pii/S259012302401750X},
	doi = {10.1016/j.rineng.2024.103498},
	abstract = {In the realm of human-machine interaction, artificial intelligence has become a powerful tool for accelerating data modeling tasks. Object detection methods have achieved outstanding results and are widely used in critical domains like autonomous driving and video surveillance. However, their adoption in high-risk applications, where errors may cause severe consequences, remains limited. Explainable Artificial Intelligence methods aim to address this issue, but many existing techniques are model-specific and designed for classification tasks, making them less effective for object detection and difficult for non-specialists to interpret. In this work we focus on model-agnostic explainability methods for object detection models and propose D-MFPP, an extension of the Morphological Fragmental Perturbation Pyramid (MFPP) technique based on segmentation-based masks to generate explanations. Additionally, we introduce D-Deletion, a novel metric combining faithfulness and localization, adapted specifically to meet the unique demands of object detectors. We evaluate these methods on real-world industrial and robotic datasets, examining the influence of parameters such as the number of masks, model size, and image resolution on the quality of explanations. Our experiments use single-stage object detection models applied to two safety-critical robotic environments: i) a shared human-robot workspace where safety is of paramount importance, and ii) an assembly area of battery kits, where safety is critical due to the potential for damage among high-risk components. Our findings evince that D-Deletion effectively gauges the performance of explanations when multiple elements of the same class appear in a scene, while D-MFPP provides a promising alternative to D-RISE when fewer masks are used.},
	urldate = {2025-11-24},
	journal = {Results in Engineering},
	author = {Andres, Alain and Martinez-Seras, Aitor and Laña, Ibai and Del Ser, Javier},
	month = dec,
	year = {2024},
	keywords = {Explainable Artificial Intelligence, Industrial robotics, Object detection, Safe Artificial Intelligence, Single-stage object detection, Trustworthy Artificial Intelligence},
	pages = {103498},
}

@article{andres_black-box_2024-1,
	title = {On the black-box explainability of object detection models for safe and trustworthy industrial applications},
	volume = {24},
	issn = {2590-1230},
	url = {https://www.sciencedirect.com/science/article/pii/S259012302401750X},
	doi = {10.1016/j.rineng.2024.103498},
	abstract = {In the realm of human-machine interaction, artificial intelligence has become a powerful tool for accelerating data modeling tasks. Object detection methods have achieved outstanding results and are widely used in critical domains like autonomous driving and video surveillance. However, their adoption in high-risk applications, where errors may cause severe consequences, remains limited. Explainable Artificial Intelligence methods aim to address this issue, but many existing techniques are model-specific and designed for classification tasks, making them less effective for object detection and difficult for non-specialists to interpret. In this work we focus on model-agnostic explainability methods for object detection models and propose D-MFPP, an extension of the Morphological Fragmental Perturbation Pyramid (MFPP) technique based on segmentation-based masks to generate explanations. Additionally, we introduce D-Deletion, a novel metric combining faithfulness and localization, adapted specifically to meet the unique demands of object detectors. We evaluate these methods on real-world industrial and robotic datasets, examining the influence of parameters such as the number of masks, model size, and image resolution on the quality of explanations. Our experiments use single-stage object detection models applied to two safety-critical robotic environments: i) a shared human-robot workspace where safety is of paramount importance, and ii) an assembly area of battery kits, where safety is critical due to the potential for damage among high-risk components. Our findings evince that D-Deletion effectively gauges the performance of explanations when multiple elements of the same class appear in a scene, while D-MFPP provides a promising alternative to D-RISE when fewer masks are used.},
	urldate = {2025-11-24},
	journal = {Results in Engineering},
	author = {Andres, Alain and Martinez-Seras, Aitor and Laña, Ibai and Del Ser, Javier},
	month = dec,
	year = {2024},
	keywords = {Explainable Artificial Intelligence, Industrial robotics, Object detection, Safe Artificial Intelligence, Single-stage object detection, Trustworthy Artificial Intelligence},
	pages = {103498},
}

@misc{lin_microsoft_2015,
	title = {Microsoft {COCO}: {Common} {Objects} in {Context}},
	shorttitle = {Microsoft {COCO}},
	url = {http://arxiv.org/abs/1405.0312},
	doi = {10.48550/arXiv.1405.0312},
	abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
	urldate = {2025-11-24},
	publisher = {arXiv},
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
	month = feb,
	year = {2015},
	note = {arXiv:1405.0312 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{geiger_are_2012,
	title = {Are we ready for autonomous driving? {The} {KITTI} vision benchmark suite},
	shorttitle = {Are we ready for autonomous driving?},
	url = {https://ieeexplore.ieee.org/document/6248074},
	doi = {10.1109/CVPR.2012.6248074},
	abstract = {Today, visual recognition systems are still rarely employed in robotics applications. Perhaps one of the main reasons for this is the lack of demanding benchmarks that mimic such scenarios. In this paper, we take advantage of our autonomous driving platform to develop novel challenging benchmarks for the tasks of stereo, optical flow, visual odometry/SLAM and 3D object detection. Our recording platform is equipped with four high resolution video cameras, a Velodyne laser scanner and a state-of-the-art localization system. Our benchmarks comprise 389 stereo and optical flow image pairs, stereo visual odometry sequences of 39.2 km length, and more than 200k 3D object annotations captured in cluttered scenarios (up to 15 cars and 30 pedestrians are visible per image). Results from state-of-the-art algorithms reveal that methods ranking high on established datasets such as Middlebury perform below average when being moved outside the laboratory to the real world. Our goal is to reduce this bias by providing challenging benchmarks with novel difficulties to the computer vision community. Our benchmarks are available online at: www.cvlibs.net/datasets/kitti},
	urldate = {2025-11-24},
	booktitle = {2012 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Geiger, Andreas and Lenz, Philip and Urtasun, Raquel},
	month = jun,
	year = {2012},
	note = {ISSN: 1063-6919},
	keywords = {Benchmark testing, Cameras, Measurement, Optical imaging, Optical sensors, Visualization},
	pages = {3354--3361},
}

@article{ren_faster_2017,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	volume = {39},
	issn = {1939-3539},
	shorttitle = {Faster {R}-{CNN}},
	url = {https://ieeexplore.ieee.org/document/7485869},
	doi = {10.1109/TPAMI.2016.2577031},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with ’attention’ mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3] , our detection system has a frame rate of 5 fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
	number = {6},
	urldate = {2025-11-24},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	month = jun,
	year = {2017},
	keywords = {Convolutional codes, Detectors, Feature extraction, Object detection, Proposals, Search problems, Training, convolutional neural network, region proposal},
	pages = {1137--1149},
}

@misc{carion_end--end_2020-1,
	title = {End-to-{End} {Object} {Detection} with {Transformers}},
	url = {http://arxiv.org/abs/2005.12872},
	doi = {10.48550/arXiv.2005.12872},
	abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
	urldate = {2025-11-24},
	publisher = {arXiv},
	author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
	month = may,
	year = {2020},
	note = {arXiv:2005.12872 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{dinga_what_2025,
	title = {What do you see? {An} {XAI} approach for {VLM}-generated map descriptions},
	volume = {5},
	shorttitle = {What do you see?},
	url = {https://ica-adv.copernicus.org/articles/5/12/2025/ica-adv-5-12-2025.html},
	doi = {10.5194/ica-adv-5-12-2025},
	abstract = {Over the last decades, significant progress has been made in enabling diverse communities to create and share cartographic maps. However, advancements in map accessibility, for blind and visually impaired users in particular, still lag behind. A critical challenge remains in generating effective and efficient text descriptions that are supported by screen-readers. Vision Language Models (VLMs) offer a promising solution, as they can produce image descriptions quickly. However, their outputs depend heavily on network architecture and prompt engineering. Further, VLMs usually are complex and outputs are difficult to interpret. To address the interpretation of outputs in particular, we propose an Explainable AI (XAI) approach using Shapley Explanations to analyze and understand the contributions of specific map regions to the text outputs generated by a VLM. Our contribution lies in applying XAI techniques to spatial data, providing a workflow to evaluate and improve the interpretability of VLM-generated map descriptions. Data and further information can be found on a corresponding GitHub repository: https://github.com/grndng/CartoXAI},
	language = {English},
	urldate = {2025-11-23},
	journal = {Advances in Cartography and GIScience of the ICA},
	author = {Dinga, Güren Tan and Schiewe, Jochen},
	month = oct,
	year = {2025},
	note = {Publisher: Copernicus GmbH},
	keywords = {CartoAI, Explainable AI (XAI), Shapley Values},
	pages = {1--7},
}

@article{dinga_what_2025-1,
	title = {What do you see? {An} {XAI} approach for {VLM}-generated map descriptions},
	volume = {5},
	url = {https://ica-adv.copernicus.org/articles/5/12/2025/},
	doi = {10.5194/ica-adv-5-12-2025},
	journal = {Advances in Cartography and GIScience of the ICA},
	author = {Dinga, G. T. and Schiewe, J.},
	year = {2025},
	pages = {12},
}

@misc{kim_interpretability_2018,
	title = {Interpretability {Beyond} {Feature} {Attribution}: {Quantitative} {Testing} with {Concept} {Activation} {Vectors} ({TCAV})},
	shorttitle = {Interpretability {Beyond} {Feature} {Attribution}},
	url = {http://arxiv.org/abs/1711.11279},
	doi = {10.48550/arXiv.1711.11279},
	abstract = {The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net's internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result--for example, how sensitive a prediction of "zebra" is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.},
	urldate = {2025-11-23},
	publisher = {arXiv},
	author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and Sayres, Rory},
	month = jun,
	year = {2018},
	note = {arXiv:1711.11279 [stat]},
	keywords = {Statistics - Machine Learning},
}

@misc{noauthor_github_nodate,
	title = {{GitHub} - jacobgil/pytorch-grad-cam: {Advanced} {AI} {Explainability} for computer vision. {Support} for {CNNs}, {Vision} {Transformers}, {Classification}, {Object} detection, {Segmentation}, {Image} similarity and more.},
	url = {https://github.com/jacobgil/pytorch-grad-cam},
	urldate = {2025-11-23},
}

@misc{noauthor_shapexplainer_nodate,
	title = {shap.{Explainer} — {SHAP} latest documentation},
	url = {https://shap-community.readthedocs.io/en/latest/generated/shap.Explainer.html},
	urldate = {2025-11-23},
}

@misc{ribeiro_marcotcrlime_2025,
	title = {marcotcr/lime},
	copyright = {BSD-2-Clause},
	url = {https://github.com/marcotcr/lime},
	abstract = {Lime: Explaining the predictions of any machine learning classifier},
	urldate = {2025-11-23},
	author = {Ribeiro, Marco Tulio Correia},
	month = nov,
	year = {2025},
	note = {original-date: 2016-03-15T22:18:10Z},
}

@misc{noauthor_explainable_nodate,
	title = {Explainable {AI} ({XAI}) {Methods}},
	url = {https://www.emergentmind.com/topics/explainable-artificial-intelligence-xai-methods},
	abstract = {Discover XAI methods that interpret complex ML models using taxonomy, formal algorithms, and evaluation metrics for practical applications.},
	urldate = {2025-11-22},
}

@article{rudin_stop_2019,
	title = {Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead},
	volume = {1},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-019-0048-x},
	doi = {10.1038/s42256-019-0048-x},
	language = {en},
	number = {5},
	urldate = {2025-11-22},
	journal = {Nature Machine Intelligence},
	author = {Rudin, Cynthia},
	month = may,
	year = {2019},
	pages = {206--215},
}

@misc{noauthor_eu_nodate,
	title = {{EU} {Artificial} {Intelligence} {Act} {\textbar} {Up}-to-date developments and analyses of the {EU} {AI} {Act}},
	url = {https://artificialintelligenceact.eu/},
	language = {en-US},
	urldate = {2025-11-22},
}

@misc{noauthor_one-stage_nodate,
	title = {One-{Stage} {Object} {Detectors} {Explained} {\textbar} {Ultralytics}},
	url = {https://www.ultralytics.com/glossary/one-stage-object-detectors},
	abstract = {Discover the speed and efficiency of one-stage object detectors like YOLO, ideal for real-time applications like robotics and surveillance.},
	language = {en},
	urldate = {2025-11-22},
}

@misc{noauthor_what_nodate,
	title = {What is {Object} {Detection} in {Computer} {Vision}?},
	url = {https://www.geeksforgeeks.org/computer-vision/what-is-object-detection-in-computer-vision/},
	abstract = {Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.},
	language = {en-US},
	urldate = {2025-11-22},
	journal = {GeeksforGeeks},
	note = {Section: Computer Vision},
}

@misc{ultralytics_computer_nodate,
	title = {Computer {Vision} {Tasks} supported by {Ultralytics} {YOLO11}},
	url = {https://docs.ultralytics.com/tasks},
	abstract = {Explore Ultralytics YOLO11 for detection, segmentation, classification, OBB, and pose estimation with high accuracy and speed. Learn how to apply each task.},
	language = {en},
	urldate = {2025-11-22},
	author = {Ultralytics},
}

@article{nauta_anecdotal_2023,
	title = {From {Anecdotal} {Evidence} to {Quantitative} {Evaluation} {Methods}: {A} {Systematic} {Review} on {Evaluating} {Explainable} {AI}},
	volume = {55},
	issn = {0360-0300, 1557-7341},
	shorttitle = {From {Anecdotal} {Evidence} to {Quantitative} {Evaluation} {Methods}},
	url = {http://arxiv.org/abs/2201.08164},
	doi = {10.1145/3583558},
	abstract = {The rising popularity of explainable artificial intelligence (XAI) to understand high-performing black boxes raised the question of how to evaluate explanations of machine learning (ML) models. While interpretability and explainability are often presented as a subjectively validated binary property, we consider it a multi-faceted concept. We identify 12 conceptual properties, such as Compactness and Correctness, that should be evaluated for comprehensively assessing the quality of an explanation. Our so-called Co-12 properties serve as categorization scheme for systematically reviewing the evaluation practices of more than 300 papers published in the last 7 years at major AI and ML conferences that introduce an XAI method. We find that 1 in 3 papers evaluate exclusively with anecdotal evidence, and 1 in 5 papers evaluate with users. This survey also contributes to the call for objective, quantifiable evaluation methods by presenting an extensive overview of quantitative XAI evaluation methods. Our systematic collection of evaluation methods provides researchers and practitioners with concrete tools to thoroughly validate, benchmark and compare new and existing XAI methods. The Co-12 categorization scheme and our identified evaluation methods open up opportunities to include quantitative metrics as optimization criteria during model training in order to optimize for accuracy and interpretability simultaneously.},
	number = {13s},
	urldate = {2025-11-04},
	journal = {ACM Computing Surveys},
	author = {Nauta, Meike and Trienes, Jan and Pathak, Shreyasi and Nguyen, Elisa and Peters, Michelle and Schmitt, Yasmin and Schlötterer, Jörg and Keulen, Maurice van and Seifert, Christin},
	month = dec,
	year = {2023},
	note = {arXiv:2201.08164 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	pages = {1--42},
}

@misc{andres_black-box_2024-2,
	title = {On the {Black}-box {Explainability} of {Object} {Detection} {Models} for {Safe} and {Trustworthy} {Industrial} {Applications}},
	url = {http://arxiv.org/abs/2411.00818},
	doi = {10.48550/arXiv.2411.00818},
	abstract = {In the realm of human-machine interaction, artificial intelligence has become a powerful tool for accelerating data modeling tasks. Object detection methods have achieved outstanding results and are widely used in critical domains like autonomous driving and video surveillance. However, their adoption in high-risk applications, where errors may cause severe consequences, remains limited. Explainable Artificial Intelligence (XAI) methods aim to address this issue, but many existing techniques are model-specific and designed for classification tasks, making them less effective for object detection and difficult for non-specialists to interpret. In this work we focus on model-agnostic XAI methods for object detection models and propose D-MFPP, an extension of the Morphological Fragmental Perturbation Pyramid (MFPP), which uses segmentation-based mask generation. Additionally, we introduce D-Deletion, a novel metric combining faithfulness and localization, adapted specifically to meet the unique demands of object detectors. We evaluate these methods on real-world industrial and robotic datasets, examining the influence of parameters such as the number of masks, model size, and image resolution on the quality of explanations. Our experiments use single-stage object detection models applied to two safety-critical robotic environments: i) a shared human-robot workspace where safety is of paramount importance, and ii) an assembly area of battery kits, where safety is critical due to the potential for damage among high-risk components. Our findings evince that D-Deletion effectively gauges the performance of explanations when multiple elements of the same class appear in the same scene, while D-MFPP provides a promising alternative to D-RISE when fewer masks are used.},
	urldate = {2025-10-10},
	publisher = {arXiv},
	author = {Andres, Alain and Martinez-Seras, Aitor and Laña, Ibai and Ser, Javier Del},
	month = oct,
	year = {2024},
	note = {arXiv:2411.00818 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@article{chen_explainable_nodate,
	title = {Explainable {Saliency}: {Articulating} {Reasoning} with {Contextual} {Prioritization}},
	abstract = {Deep saliency models, which predict what parts of an image capture our attention, are often like black boxes. This limits their use, especially in areas where understanding why a model makes a decision is crucial. Our research tackles this challenge by developing an explainable saliency (XSal) model that not only identiﬁes what is important in an image, but also explains its choices in a way that makes sense to humans. We achieve this by using vision-language models to reason about images and by focusing the model’s attention on the most crucial information using a contextual prioritization mechanism. Unlike prior approaches that rely on ﬁxation descriptions or soft-attention based semantic aggregation, our method directly models the reasoning steps involved in saliency prediction, generating selectively prioritized explanations clarify why speciﬁc regions are prioritized. Comprehensive evaluations demonstrate the effectiveness of our model in generating high-quality saliency maps and coherent, contextually relevant explanations. This research is a step towards more transparent and trustworthy AI systems that can help us understand and navigate the world around us.},
	language = {en},
	author = {Chen, Nuo and Jiang, Ming and Zhao, Qi},
}

@inproceedings{feldhus_saliency_2023,
	address = {Toronto, Canada},
	title = {Saliency {Map} {Verbalization}: {Comparing} {Feature} {Importance} {Representations} from {Model}-free and {Instruction}-based {Methods}},
	shorttitle = {Saliency {Map} {Verbalization}},
	url = {https://aclanthology.org/2023.nlrse-1.4},
	doi = {10.18653/v1/2023.nlrse-1.4},
	language = {en},
	urldate = {2025-10-04},
	booktitle = {Proceedings of the 1st {Workshop} on {Natural} {Language} {Reasoning} and {Structured} {Explanations} ({NLRSE})},
	publisher = {Association for Computational Linguistics},
	author = {Feldhus, Nils and Hennig, Leonhard and Nasert, Maximilian and Ebert, Christopher and Schwarzenberg, Robert and Mller, Sebastian},
	year = {2023},
	pages = {30--46},
}

@misc{wu_usable_2025,
	title = {Usable {XAI}: 10 {Strategies} {Towards} {Exploiting} {Explainability} in the {LLM} {Era}},
	shorttitle = {Usable {XAI}},
	url = {http://arxiv.org/abs/2403.08946},
	doi = {10.48550/arXiv.2403.08946},
	abstract = {Explainable AI (XAI) refers to techniques that provide human-understandable insights into the workings of AI models. Recently, the focus of XAI is being extended toward explaining Large Language Models (LLMs). This extension calls for a significant transformation in the XAI methodologies for two reasons. First, many existing XAI methods cannot be directly applied to LLMs due to their complexity and advanced capabilities. Second, as LLMs are increasingly deployed in diverse applications, the role of XAI shifts from merely opening the ``black box'' to actively enhancing the productivity and applicability of LLMs in real-world settings. Meanwhile, the conversation and generation abilities of LLMs can reciprocally enhance XAI. Therefore, in this paper, we introduce Usable XAI in the context of LLMs by analyzing (1) how XAI can explain and improve LLM-based AI systems and (2) how XAI techniques can be improved by using LLMs. We introduce 10 strategies, introducing the key techniques for each and discussing their associated challenges. We also provide case studies to demonstrate how to obtain and leverage explanations. The code used in this paper can be found at: https://github.com/JacksonWuxs/UsableXAI\_LLM.},
	urldate = {2025-10-04},
	publisher = {arXiv},
	author = {Wu, Xuansheng and Zhao, Haiyan and Zhu, Yaochen and Shi, Yucheng and Yang, Fan and Hu, Lijie and Liu, Tianming and Zhai, Xiaoming and Yao, Wenlin and Li, Jundong and Du, Mengnan and Liu, Ninghao},
	month = may,
	year = {2025},
	note = {arXiv:2403.08946 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@article{minh_explainable_2022,
	title = {Explainable artificial intelligence: a comprehensive review},
	volume = {55},
	issn = {1573-7462},
	shorttitle = {Explainable artificial intelligence},
	url = {https://doi.org/10.1007/s10462-021-10088-y},
	doi = {10.1007/s10462-021-10088-y},
	abstract = {Thanks to the exponential growth in computing power and vast amounts of data, artificial intelligence (AI) has witnessed remarkable developments in recent years, enabling it to be ubiquitously adopted in our daily lives. Even though AI-powered systems have brought competitive advantages, the black-box nature makes them lack transparency and prevents them from explaining their decisions. This issue has motivated the introduction of explainable artificial intelligence (XAI), which promotes AI algorithms that can show their internal process and explain how they made decisions. The number of XAI research has increased significantly in recent years, but there lacks a unified and comprehensive review of the latest XAI progress. This review aims to bridge the gap by discovering the critical perspectives of the rapidly growing body of research associated with XAI. After offering the readers a solid XAI background, we analyze and review various XAI methods, which are grouped into (i) pre-modeling explainability, (ii) interpretable model, and (iii) post-modeling explainability. We also pay attention to the current methods that dedicate to interpret and analyze deep learning methods. In addition, we systematically discuss various XAI challenges, such as the trade-off between the performance and the explainability, evaluation methods, security, and policy. Finally, we show the standard approaches that are leveraged to deal with the mentioned challenges.},
	language = {en},
	number = {5},
	urldate = {2025-10-04},
	journal = {Artificial Intelligence Review},
	author = {Minh, Dang and Wang, H. Xiang and Li, Y. Fen and Nguyen, Tan N.},
	month = jun,
	year = {2022},
	keywords = {Black-box models, Deep learning, Explainable artificial intelligence, Interpretability, Machine learning},
	pages = {3503--3568},
}

@misc{natarajan_vale_2024,
	title = {{VALE}: {A} {Multimodal} {Visual} and {Language} {Explanation} {Framework} for {Image} {Classifiers} using {eXplainable} {AI} and {Language} {Models}},
	shorttitle = {{VALE}},
	url = {http://arxiv.org/abs/2408.12808},
	doi = {10.48550/arXiv.2408.12808},
	abstract = {Deep Neural Networks (DNNs) have revolutionized various fields by enabling task automation and reducing human error. However, their internal workings and decision-making processes remain obscure due to their black box nature. Consequently, the lack of interpretability limits the application of these models in high-risk scenarios. To address this issue, the emerging field of eXplainable Artificial Intelligence (XAI) aims to explain and interpret the inner workings of DNNs. Despite advancements, XAI faces challenges such as the semantic gap between machine and human understanding, the trade-off between interpretability and performance, and the need for context-specific explanations. To overcome these limitations, we propose a novel multimodal framework named VALE Visual and Language Explanation. VALE integrates explainable AI techniques with advanced language models to provide comprehensive explanations. This framework utilizes visual explanations from XAI tools, an advanced zero-shot image segmentation model, and a visual language model to generate corresponding textual explanations. By combining visual and textual explanations, VALE bridges the semantic gap between machine outputs and human interpretation, delivering results that are more comprehensible to users. In this paper, we conduct a pilot study of the VALE framework for image classification tasks. Specifically, Shapley Additive Explanations (SHAP) are used to identify the most influential regions in classified images. The object of interest is then extracted using the Segment Anything Model (SAM), and explanations are generated using state-of-the-art pre-trained Vision-Language Models (VLMs). Extensive experimental studies are performed on two datasets: the ImageNet dataset and a custom underwater SONAR image dataset, demonstrating VALEs real-world applicability in underwater image classification.},
	urldate = {2025-10-04},
	publisher = {arXiv},
	author = {Natarajan, Purushothaman and Nambiar, Athira},
	month = aug,
	year = {2024},
	note = {arXiv:2408.12808 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{sahatova_overview_2022,
	title = {An {Overview} and {Comparison} of {XAI} {Methods} for {Object} {Detection} in {Computer} {Tomography}},
	volume = {212},
	issn = {18770509},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050922016969},
	doi = {10.1016/j.procs.2022.11.005},
	abstract = {Modern hardware and software developments in the medical field generate massive amounts of data that clinicians need to analyze. Many solutions based on deep learning have been introduced to support the diagnostic process. Nonetheless, the transparency and reasoning of such systems are important for medical practices that limit the application of artificial intelligence techniques that work in 'black box' scenarios. The purpose of this paper is to present algorithms that allow interpretation of the complex structure of models used in object detection. Based on the ablation study results, a detailed analysis of the advantages and disadvantages of the chosen methods has been provided. Infidelity and consistency metrics were used to assess the algorithms of explanation.},
	language = {en},
	urldate = {2025-10-04},
	journal = {Procedia Computer Science},
	author = {Sahatova, Kseniya and Balabaeva, Ksenia},
	year = {2022},
	pages = {209--219},
}

@article{akula_natural_nodate,
	title = {Natural {Language} {Interaction} with {Explainable} {AI} {Models}},
	abstract = {This paper presents an explainable AI (XAI) system that provides explanations for its predictions. The system consists of two key components – namely, the prediction And-Or graph (AOG) model for recognizing and localizing concepts of interest in input data, and the XAI model for providing explanations to the user about the AOG’s predictions. In this work, we focus on the XAI model speciﬁed to interact with the user in natural language, whereas the AOG’s predictions are considered given and represented by the corresponding parse graphs (pg’s) of the AOG. Our XAI model takes pg’s as input and provides answers to the user’s questions using the following types of reasoning: direct evidence (e.g., detection scores), part-based inference (e.g., detected parts provide evidence for the concept asked), and other evidences from spatiotemporal context (e.g., constraints from the spatiotemporal surround). We identify several correlations between user’s questions and the XAI answers using Youtube Action dataset.},
	language = {en},
	author = {Akula, Arjun R and Todorovic, Sinisa and Chai, Joyce Y and Zhu, Song-Chun},
}

@article{tjoa_survey_2021,
	title = {A {Survey} on {Explainable} {Artificial} {Intelligence} ({XAI}): {Toward} {Medical} {XAI}},
	volume = {32},
	issn = {2162-2388},
	shorttitle = {A {Survey} on {Explainable} {Artificial} {Intelligence} ({XAI})},
	url = {https://ieeexplore.ieee.org/document/9233366/},
	doi = {10.1109/TNNLS.2020.3027314},
	abstract = {Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning (DL). Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example, the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the blackbox nature of the DL is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them. The different categories show different dimensions in interpretability research, from approaches that provide “obviously” interpretable information to the studies of complex patterns. By applying the same categorization to interpretability in medical research, it is hoped that: 1) clinicians and practitioners can subsequently approach these methods with caution; 2) insight into interpretability will be born with more considerations for medical practices; and 3) initiatives to push forward data-based, mathematically grounded, and technically grounded medical education are encouraged.},
	number = {11},
	urldate = {2025-10-04},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Tjoa, Erico and Guan, Cuntai},
	month = nov,
	year = {2021},
	keywords = {Artificial intelligence, Explainable artificial intelligence (XAI), Machine learning, Machine learning algorithms, Medical information systems, interpretability, machine learning (ML), medical information system, survey},
	pages = {4793--4813},
}

@article{afroogh_trust_2024,
	title = {Trust in {AI}: progress, challenges, and future directions},
	volume = {11},
	issn = {2662-9992},
	shorttitle = {Trust in {AI}},
	url = {https://www.nature.com/articles/s41599-024-04044-8},
	doi = {10.1057/s41599-024-04044-8},
	abstract = {We conducted an inclusive and systematic review of academic papers, reports, case studies, and trust frameworks in AI, written in English. Given that there is not a speciﬁc database on trust in AI in particular, we used the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) framework to develop a protocol in this review (Fig. 1). In order to conduct a comprehensive review of the relevant studies, we followed two approaches. First, we manually searched for the most related papers on trust in AI: 19 papers were identiﬁed through the online search after the removal of duplicate ﬁles. Secondly, we fulﬁlled a keyword-based search (using the http://scholar.google.com search engine) to collect all relevant papers on the topic. This search was accomplished using the following keyword phrases: (1) “trust + AI” which provided 19 relevant result pages of Google Scholar, (2) “trust + Artiﬁcial + Intelligence” for which the ﬁrst ﬁve result pages were reviewed, (3) “trustworthy + AI,” for which the ﬁrst 15 result pages were reviewed; and (4) “trustworthy + Artiﬁcial + Intelligence,” for which the ﬁrst 13 result pages of Google Scholar were reviewed.},
	language = {en},
	number = {1},
	urldate = {2025-10-03},
	journal = {Humanities and Social Sciences Communications},
	author = {Afroogh, Saleh and Akbari, Ali and Malone, Emmie and Kargar, Mohammadali and Alambeigi, Hananeh},
	month = nov,
	year = {2024},
	pages = {1568},
}

@article{moradi_model-agnostic_2024,
	title = {Model-agnostic explainable artificial intelligence for object detection in image data},
	volume = {137},
	issn = {09521976},
	url = {http://arxiv.org/abs/2303.17249},
	doi = {10.1016/j.engappai.2024.109183},
	abstract = {In recent years, deep neural networks have been widely used for building high-performance Artificial Intelligence (AI) systems for computer vision applications. Object detection is a fundamental task in computer vision, which has been greatly progressed through developing large and intricate AI models. However, the lack of transparency is a big challenge that may not allow the widespread adoption of these models. Explainable artificial intelligence is a field of research where methods are developed to help users understand the behavior, decision logics, and vulnerabilities of AI systems. Previously, few explanation methods were developed for object detection based on random masking. However, random masks may raise some issues regarding the actual importance of pixels within an image. In this paper, we design and implement a black-box explanation method named Black-box Object Detection Explanation by Masking (BODEM) through adopting a hierarchical random masking approach for object detection systems. We propose a hierarchical random masking framework in which coarse-grained masks are used in lower levels to find salient regions within an image, and fine-grained mask are used to refine the salient regions in higher levels. Experimentations on various object detection datasets and models showed that BODEM can effectively explain the behavior of object detectors. Moreover, our method outperformed Detector Randomized Input Sampling for Explanation (D-RISE) and Local Interpretable Model-agnostic Explanations (LIME) with respect to different quantitative measures of explanation effectiveness. The experimental results demonstrate that BODEM can be an effective method for explaining and validating object detection systems in black-box testing scenarios.},
	urldate = {2025-10-03},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Moradi, Milad and Yan, Ke and Colwell, David and Samwald, Matthias and Asgari, Rhona},
	month = nov,
	year = {2024},
	note = {arXiv:2303.17249 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	pages = {109183},
}

@article{sahatova_overview_2022-1,
	title = {An {Overview} and {Comparison} of {XAI} {Methods} for {Object} {Detection} in {Computer} {Tomography}},
	volume = {212},
	issn = {18770509},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050922016969},
	doi = {10.1016/j.procs.2022.11.005},
	abstract = {Modern hardware and software developments in the medical field generate massive amounts of data that clinicians need to analyze. Many solutions based on deep learning have been introduced to support the diagnostic process. Nonetheless, the transparency and reasoning of such systems are important for medical practices that limit the application of artificial intelligence techniques that work in 'black box' scenarios. The purpose of this paper is to present algorithms that allow interpretation of the complex structure of models used in object detection. Based on the ablation study results, a detailed analysis of the advantages and disadvantages of the chosen methods has been provided. Infidelity and consistency metrics were used to assess the algorithms of explanation.},
	language = {en},
	urldate = {2025-10-03},
	journal = {Procedia Computer Science},
	author = {Sahatova, Kseniya and Balabaeva, Ksenia},
	year = {2022},
	pages = {209--219},
}

@misc{cambria_xai_2024,
	title = {{XAI} meets {LLMs}: {A} {Survey} of the {Relation} between {Explainable} {AI} and {Large} {Language} {Models}},
	shorttitle = {{XAI} meets {LLMs}},
	url = {http://arxiv.org/abs/2407.15248},
	doi = {10.48550/arXiv.2407.15248},
	abstract = {In this survey, we address the key challenges in Large Language Models (LLM) research, focusing on the importance of interpretability. Driven by increasing interest from AI and business sectors, we highlight the need for transparency in LLMs. We examine the dual paths in current LLM research and eXplainable Artificial Intelligence (XAI): enhancing performance through XAI and the emerging focus on model interpretability. Our paper advocates for a balanced approach that values interpretability equally with functional advancements. Recognizing the rapid development in LLM research, our survey includes both peer-reviewed and preprint (arXiv) papers, offering a comprehensive overview of XAI's role in LLM research. We conclude by urging the research community to advance both LLM and XAI fields together.},
	urldate = {2025-09-30},
	publisher = {arXiv},
	author = {Cambria, Erik and Malandri, Lorenzo and Mercorio, Fabio and Nobani, Navid and Seveso, Andrea},
	month = jul,
	year = {2024},
	note = {arXiv:2407.15248 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{ozenc_evaluation_nodate,
	title = {An {Evaluation} of {XAI} {Methods} for {Object} {Detection} in {Satellite} {Images} using {YOLOv5}},
	url = {https://avesis.kocaeli.edu.tr/yayin/04959bc0-60f9-4096-a2e8-da039a2c5065/an-evaluation-of-xai-methods-for-object-detection-in-satellite-images-using-yolov5},
	abstract = {In recent years, deep learning based approaches
have gained widespread adoption in Earth observation and remote sensing,
mirroring their success in numerous other domains. However, unlike approaches
based on physical models, deep learning methods operate as black boxes,
concealing internal processes influencing final decisions. This lack of
transparency poses a challenge, particularly in applications where
interpretability is paramount, as outputs generated by these approaches cannot
be fully trusted or verified. Explainable Artificial Intelligence (XAI) aims to
make the deep learning processes and their outputs more interpretable for
researchers and end users. The purpose of this study is to investigate and
evaluate the performance of various XAI methodologies for post-hoc
explainability of object detection in satellite images using deep learning. Class-activation
mapping (CAM) based XAI methods, namely GradCAM, GradCAM++, EigenCAM, ScoreCAM,
and LayerCAM, are used for post-hoc explainability, following target detection
by You Look Only Once (YOLO) algorithm. Experimental results show that each
method provides considerably different saliency maps, which may be used for qualitative
performance analysis of the interpretability provided by these methods. However,
in a large dataset, a qualitative analysis by itself may be subjective and
misleading. As such, an evaluation framework tailored for remote sensing
applications is adopted to evaluate the interpretability performances of these
XAI methods quantitatively. The findings provide an important step towards
understanding the role and effectiveness of these XAI methods for
interpretability of object detection for remote sensing.},
	language = {en},
	urldate = {2025-09-30},
	author = {Özenç, Uğur and Ertürk, A. L. P.},
}

@article{liu_human_nodate,
	title = {Human {Attention}-{Guided} {Explainable} {AI} for {Object} {Detection}},
	abstract = {Although object detection AI plays an important role in many critical systems, corresponding Explainable AI (XAI) methods remain very limited. Here we first developed FullGrad-CAM and FullGrad-CAM++ by extending traditional gradient-based methods to generate object-specific explanations with higher plausibility. Since human attention may reflect features more interpretable to humans, we explored the possibility to use it as guidance to learn how to combine the explanatory information in the detector model to best present as an XAI saliency map that is interpretable (plausible) to humans. Interestingly, we found that human attention maps had higher faithfulness for explaining the detector model than existing saliency-based XAI methods. By using trainable activation functions and smoothing kernels to maximize the XAI saliency map similarity to human attention maps, the generated map had higher faithfulness and plausibility than both existing XAI methods and human attention maps. The learned functions were modelspecific, well generalizable to other databases.},
	language = {en},
	author = {Liu, Guoyang and Zhang, Jindi and Chan, Antoni B and Hsiao, Janet H},
}

@misc{nguyen_odexai_2025,
	title = {{ODExAI}: {A} {Comprehensive} {Object} {Detection} {Explainable} {AI} {Evaluation}},
	shorttitle = {{ODExAI}},
	url = {http://arxiv.org/abs/2504.19249},
	doi = {10.48550/arXiv.2504.19249},
	abstract = {Explainable Artificial Intelligence (XAI) techniques for interpreting object detection models remain in an early stage, with no established standards for systematic evaluation. This absence of consensus hinders both the comparative analysis of methods and the informed selection of suitable approaches. To address this gap, we introduce the Object Detection Explainable AI Evaluation (ODExAI), a comprehensive framework designed to assess XAI methods in object detection based on three core dimensions: localization accuracy, faithfulness to model behavior, and computational complexity. We benchmark a set of XAI methods across two widely used object detectors (YOLOX and Faster R-CNN) and standard datasets (MS-COCO and PASCAL VOC). Empirical results demonstrate that region-based methods (e.g., D-CLOSE) achieve strong localization (PG = 88.49\%) and high model faithfulness (OA = 0.863), though with substantial computational overhead (Time = 71.42s). On the other hand, CAM-based methods (e.g., G-CAME) achieve superior localization (PG = 96.13\%) and significantly lower runtime (Time = 0.54s), but at the expense of reduced faithfulness (OA = 0.549). These findings demonstrate critical trade-offs among existing XAI approaches and reinforce the need for task-specific evaluation when deploying them in object detection pipelines. Our implementation and evaluation benchmarks are publicly available at: https://github.com/Analytics-Everywhere-Lab/odexai.},
	urldate = {2025-09-30},
	publisher = {arXiv},
	author = {Nguyen, Loc Phuc Truong and Nguyen, Hung Truong Thanh and Cao, Hung},
	month = apr,
	year = {2025},
	note = {arXiv:2504.19249 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{mankodiya_od-xai_2022,
	title = {{OD}-{XAI}: {Explainable} {AI}-{Based} {Semantic} {Object} {Detection} for {Autonomous} {Vehicles}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	shorttitle = {{OD}-{XAI}},
	url = {https://www.mdpi.com/2076-3417/12/11/5310},
	doi = {10.3390/app12115310},
	abstract = {In recent years, artificial intelligence (AI) has become one of the most prominent fields in autonomous vehicles (AVs). With the help of AI, the stress levels of drivers have been reduced, as most of the work is executed by the AV itself. With the increasing complexity of models, explainable artificial intelligence (XAI) techniques work as handy tools that allow naive people and developers to understand the intricate workings of deep learning models. These techniques can be paralleled to AI to increase their interpretability. One essential task of AVs is to be able to follow the road. This paper attempts to justify how AVs can detect and segment the road on which they are moving using deep learning (DL) models. We trained and compared three semantic segmentation architectures for the task of pixel-wise road detection. Max IoU scores of 0.9459 and 0.9621 were obtained on the train and test set. Such DL algorithms are called “black box models” as they are hard to interpret due to their highly complex structures. Integrating XAI enables us to interpret and comprehend the predictions of these abstract models. We applied various XAI methods and generated explanations for the proposed segmentation model for road detection in AVs.},
	language = {en},
	number = {11},
	urldate = {2025-09-30},
	journal = {Applied Sciences},
	author = {Mankodiya, Harsh and Jadav, Dhairya and Gupta, Rajesh and Tanwar, Sudeep and Hong, Wei-Chiang and Sharma, Ravi},
	month = jan,
	year = {2022},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {DL, KITTI dataset, ResNet, SegNet, autonomous vehicles, black box, explainable AI, object detection, semantic segmentation},
	pages = {5310},
}

@article{singhal_explainable_2024,
	title = {Explainable {Artificial} {Intelligence} ({XAI}) {Model} for {Cancer} {Image} {Classification}},
	volume = {141},
	issn = {1526-1506},
	url = {https://www.techscience.com/CMES/v141n1/57687},
	doi = {10.32604/cmes.2024.051363},
	abstract = {The use of Explainable Artificial Intelligence (XAI) models becomes increasingly important for making decisions in smart healthcare environments. It is to make sure that decisions are based on trustworthy algorithms and that healthcare workers understand the decisions made by these algorithms. These models can potentially enhance interpretability and explainability in decision-making processes that rely on artificial intelligence. Nevertheless, the intricate nature of the healthcare field necessitates the utilization of sophisticated models to classify cancer images. This research presents an advanced investigation of XAI models to classify cancer images. It describes the different levels of explainability and interpretability associated with XAI models and the challenges faced in deploying them in healthcare applications. In addition, this study proposes a novel framework for cancer image classification that incorporates XAI models with deep learning and advanced medical imaging techniques. The proposed model integrates several techniques, including end-to-end explainable evaluation, rule-based explanation, and useradaptive explanation. The proposed XAI reaches 97.72\% accuracy, 90.72\% precision, 93.72\% recall, 96.72\% F1score, 9.55\% FDR, 9.66\% FOR, and 91.18\% DOR. It will discuss the potential applications of the proposed XAI models in the smart healthcare environment. It will help ensure trust and accountability in AI-based decisions, which is essential for achieving a safe and reliable smart healthcare environment.},
	language = {en},
	number = {1},
	urldate = {2025-09-30},
	journal = {Computer Modeling in Engineering \& Sciences},
	author = {Singhal, Amit and Agrawal, Krishna Kant and Quezada, Angeles and Aguiñaga, Adrian Rodriguez and Jiménez, Samantha and Yadav, Satya Prakash},
	year = {2024},
	pages = {401--441},
}

@misc{song_ml_2025,
	title = {{ML} {Model} {Explainability}: {SHAP} vs. {LIME}},
	shorttitle = {{ML} {Model} {Explainability}},
	url = {https://mljourney.com/ml-model-explainability-shap-vs-lime/},
	abstract = {Discover the key differences between SHAP and LIME for ML model explainability. Compare features, use cases, and implementation...},
	language = {en-US},
	urldate = {2025-09-30},
	journal = {ML Journey},
	author = {Song, Peter},
	month = jun,
	year = {2025},
}

@misc{noauthor_llama-modelsmodelsllama3_2_nodate,
	title = {llama-models/models/llama3\_2 at main · meta-llama/llama-models},
	url = {https://github.com/meta-llama/llama-models/tree/main/models/llama3_2},
	abstract = {Utilities intended for use with Llama models. Contribute to meta-llama/llama-models development by creating an account on GitHub.},
	language = {en},
	urldate = {2025-09-17},
	journal = {GitHub},
}

@misc{noauthor_meta-llamallama-32-11b-vision_2024,
	title = {meta-llama/{Llama}-3.2-{11B}-{Vision} · {Hugging} {Face}},
	url = {https://huggingface.co/meta-llama/Llama-3.2-11B-Vision},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2025-09-17},
	month = dec,
	year = {2024},
}

@misc{noauthor_what_nodate-1,
	title = {What are {Vision}-{Language} {Models}?},
	url = {https://www.nvidia.com/en-us/glossary/vision-language-models/},
	abstract = {Check NVIDIA Glossary for more details.},
	language = {en-us},
	urldate = {2025-09-17},
	journal = {NVIDIA},
}

@misc{noauthor_was_2025,
	title = {Was sind {Vision} {Language} {Models} ({VLMs})? {\textbar} {IBM}},
	shorttitle = {Was sind {Vision} {Language} {Models} ({VLMs})?},
	url = {https://www.ibm.com/de-de/think/topics/vision-language-models},
	abstract = {Vision Language Models (VLMs) sind Modelle der künstlichen Intelligenz (KI), die Funktionen der Computer Vision und der Verarbeitung natürlicher Sprache (NLP) miteinander verbinden.},
	language = {de},
	urldate = {2025-09-17},
	month = feb,
	year = {2025},
}

@misc{noauthor_guide_nodate,
	title = {A {Guide} to {Object} {Detection} with {Vision}-{Language} {Models} {\textbar} {DigitalOcean}},
	url = {https://www.digitalocean.com/community/conceptual-articles/hands-on-guide-to-object-detection-with-vision-language-models},
	abstract = {Explore how Vision-Language Models are transforming object detection with practical techniques, real-world use cases, and hands-on implementation tips.},
	language = {en},
	urldate = {2025-09-17},
}

@misc{noauthor_pa_trustworthy_ai_nodate,
	title = {{PA}\_Trustworthy\_AI},
	url = {https://de.overleaf.com/project/68c3d006685a3f5c391dd0ce},
	abstract = {Ein einfach bedienbarer Online-LaTeX-Editor. Keine Installation notwendig, Zusammenarbeit in Echtzeit, Versionskontrolle, Hunderte von LaTeX-Vorlagen und mehr},
	language = {de},
	urldate = {2025-09-16},
}

@misc{noauthor_vision_nodate,
	title = {Vision {Language} {Models} {Explained}},
	url = {https://huggingface.co/blog/vlms},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2025-09-14},
}

@misc{noauthor_open_nodate,
	title = {Open {Object} {Detection} {Leaderboard} - a {Hugging} {Face} {Space} by hf-vision},
	url = {https://huggingface.co/spaces/hf-vision/object_detection_leaderboard},
	abstract = {Request evaluation for a new model on the COCO validation 2017 dataset. Enter the model name to submit your request; you'll get a confirmation message if the request is accepted.},
	urldate = {2025-09-14},
}

@misc{noauthor_object_nodate,
	title = {Object {Detection} {Leaderboard}},
	url = {https://huggingface.co/blog/object-detection-leaderboard},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2025-09-14},
}

@misc{noauthor_coco_nodate,
	title = {{COCO} - {Common} {Objects} in {Context}},
	url = {https://cocodataset.org/#home},
	urldate = {2025-09-12},
}
